{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../dataset/train.csv')\n",
    "test = pd.read_csv('../dataset/test.csv')\n",
    "x_train,y_train,x_test,y_test = np.array(train['x']),np.array(train['y']),np.array(test['x']),np.array(test['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train) == len(y_train)\n",
    "x_train = np.linspace(0.0,10.0,700)\n",
    "y_train = np.linspace(0.0,10.0,700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing hyper parameters\n",
    "m = tf.Variable(0.5)\n",
    "c = tf.Variable(0.2)\n",
    "batch_size = 100;\n",
    "xPh = tf.placeholder(tf.float32,[batch_size])\n",
    "yPh = tf.placeholder(tf.float32,[batch_size])\n",
    "y_hat = m * xPh + 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss minimization\n",
    "loss = tf.reduce_sum(tf.square(y_hat-yPh))\n",
    "optimize = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss [ 24.  50.  15.  38.  87.  36.  12.  81.  25.   5.  16.  16.  24.  39.  54.\n",
      "  60.  26.  73.  29.  31.  68.  87.  58.  54.  84.  58.  49.  20.  90.  48.\n",
      "   4.  25.  42.   0.  60.  93.  39.   7.  21.  68.  84.   0.  58.  19.  36.\n",
      "  19.  59.  51.  19.  33.  85.  44.   5.  59.  14.   9.  75.  69.  10.  17.\n",
      "  58.  74.  21.  51.  19.  50.  24.   0.  12.  75.  21.  64.   5.  58.  32.\n",
      "  41.   7.   4.   5.  49.  90.   3.  11.  32.  83.  25.  83.  26.  76.  95.\n",
      "  53.  77.  42.  25.  54.  55.   0.  73.  35.  86.] ++ 61748.2 1\n",
      "loss [  90.   13.   46.   46.   32.    8.   71.   28.   24.   56.   49.   79.\n",
      "   90.   89.   41.   27.   58.   26.   31.   70.   71.   39.    7.   48.\n",
      "   56.   45.   41.    3.   37.   24.   68.   47.   27.   68.   74.   95.\n",
      "   79.   21.   95.   54.   56.   80.   26.   25.    8.   95.   94.   54.\n",
      "    7.   99.   36.   48.   65.   42.   93.   86.   26.   51.  100.   94.\n",
      "    6.   24.   75.    7.   53.   73.   16.   80.   77.   89.   80.   55.\n",
      "   19.   56.   47.   56.    2.   82.   57.   44.   26.   52.   41.   44.\n",
      "    3.   31.   97.   21.   17.    7.   61.   10.   52.   10.   65.   71.\n",
      "    4.   24.   26.   51.] ++ 1.9073e+10 2\n",
      "loss [  4.20000000e+01   6.20000000e+01   7.40000000e+01   7.70000000e+01\n",
      "   3.00000000e+00   5.00000000e+01   2.40000000e+01   3.70000000e+01\n",
      "   5.80000000e+01   5.20000000e+01   2.70000000e+01   1.40000000e+01\n",
      "   1.00000000e+02   3.53015737e+03   7.20000000e+01   5.00000000e+00\n",
      "   7.10000000e+01   5.40000000e+01   8.40000000e+01   4.20000000e+01\n",
      "   5.40000000e+01   7.40000000e+01   5.40000000e+01   5.30000000e+01\n",
      "   7.80000000e+01   9.70000000e+01   4.90000000e+01   7.10000000e+01\n",
      "   4.80000000e+01   5.10000000e+01   8.90000000e+01   9.90000000e+01\n",
      "   9.30000000e+01   4.90000000e+01   1.80000000e+01   6.50000000e+01\n",
      "   8.30000000e+01   1.00000000e+02   4.10000000e+01   5.20000000e+01\n",
      "   2.90000000e+01   9.70000000e+01   7.00000000e+00   5.10000000e+01\n",
      "   5.80000000e+01   5.00000000e+01   6.70000000e+01   8.90000000e+01\n",
      "   7.60000000e+01   3.50000000e+01   9.90000000e+01   3.10000000e+01\n",
      "   5.20000000e+01   1.10000000e+01   6.60000000e+01   5.00000000e+01\n",
      "   3.90000000e+01   6.00000000e+01   3.50000000e+01   5.30000000e+01\n",
      "   1.40000000e+01   4.90000000e+01   1.60000000e+01   7.60000000e+01\n",
      "   1.30000000e+01   5.10000000e+01   7.00000000e+01   9.80000000e+01\n",
      "   8.60000000e+01   1.00000000e+02   4.60000000e+01   5.10000000e+01\n",
      "   5.00000000e+01   9.10000000e+01   4.80000000e+01   8.10000000e+01\n",
      "   3.80000000e+01   4.00000000e+01   7.90000000e+01   9.60000000e+01\n",
      "   6.00000000e+01   7.00000000e+01   4.40000000e+01   1.10000000e+01\n",
      "   6.00000000e+00   5.00000000e+00   7.20000000e+01   5.50000000e+01\n",
      "   9.50000000e+01   4.10000000e+01   2.50000000e+01   1.00000000e+00\n",
      "   5.50000000e+01   4.00000000e+00   4.80000000e+01   5.50000000e+01\n",
      "   7.50000000e+01   6.80000000e+01   1.00000000e+02   2.50000000e+01] ++ nan 3\n",
      "loss [ 75.  34.  38.  92.  21.  88.  75.  76.  44.  10.  21.  16.  32.  13.  26.\n",
      "  70.  77.  77.  88.  35.  24.  17.  91.  32.  36.  89.  69.  30.   6.  22.\n",
      "  67.   9.  74.  50.  85.   3.   0.  59.  62.  17.  90.  23.  19.  93.  14.\n",
      "  58.  87.  37.  20.  35.  63.  56.  62.  98.  90.  51.  93.  22.  38.  13.\n",
      "  98.  99.  31.  94.  73.  37.  23.  11.  88.  47.  79.  91.  71.  10.  39.\n",
      "  92.  99.  28.  32.  32.  75.  99.  27.  64.  98.  38.  46.  13.  96.   9.\n",
      "  34.  49.   1.  50.  94.  27.  20.  12.  45.  91.] ++ nan 4\n",
      "loss [ 61.  10.  47.  33.  84.  24.  48.  48.   9.  93.  99.   8.  20.  38.  78.\n",
      "  81.  42.  95.  78.  44.  68.  87.  58.  52.  26.  75.  48.  71.  77.  34.\n",
      "  24.  70.  29.  76.  98.  28.  87.   9.  87.  33.  64.  17.  49.  95.  75.\n",
      "  89.  81.  25.  47.  50.   5.  68.  84.   8.  41.  26.  89.  78.  34.  92.\n",
      "  27.  12.   2.  22.   0.  26.  50.  84.  70.  66.  42.  19.  94.  71.  19.\n",
      "  16.  49.  29.  29.  86.  50.  86.  30.  23.  20.  16.  57.   8.   8.  62.\n",
      "  55.  30.  86.  62.  51.  61.  86.  61.  21.  81.] ++ nan 5\n",
      "loss [  97.    5.   61.   47.   98.   30.   63.    0.  100.   18.   30.   98.\n",
      "   16.   22.   55.   43.   75.   91.   46.   85.   55.   36.   49.   94.\n",
      "   43.   22.   37.   24.   95.   61.   75.   68.   58.    5.   53.   80.\n",
      "   83.   25.   34.   26.   90.   60.   49.   19.   92.   29.    8.   57.\n",
      "   29.   19.   81.   50.   15.   70.   39.   43.   21.   98.   86.   16.\n",
      "   25.   31.   93.   67.   49.   25.   88.   54.   21.    8.   32.   35.\n",
      "   67.   90.   59.   15.   67.   42.   44.   77.   68.   36.   11.   10.\n",
      "   65.   98.   98.   49.   31.   56.   70.   91.   25.   54.   39.   91.\n",
      "    3.   22.    2.    2.] ++ nan 6\n",
      "loss [  65.   71.   42.   76.   43.    8.   86.   87.    3.   58.   62.   89.\n",
      "   95.   28.    0.    1.   49.   21.   46.   11.   89.   37.   29.   44.\n",
      "   96.   16.   74.   35.   42.   16.   56.   18.  100.   54.   92.   63.\n",
      "   81.   73.   48.    1.   85.   14.   25.   45.   98.   97.   58.   93.\n",
      "   88.   89.   47.    6.   34.   30.   16.   86.   40.   52.   15.    4.\n",
      "   95.   99.   35.   58.   10.   16.   53.   58.   42.   24.   84.   64.\n",
      "   12.   61.   75.   15.  100.   43.   13.   48.   45.   52.   34.   30.\n",
      "   65.  100.   67.   99.   45.   87.   73.    9.   81.   72.   81.   58.\n",
      "   93.   82.   66.   97.] ++ nan 7\n",
      "nan 0.2\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epochs = 7\n",
    "    x_init = 0\n",
    "    for i in range(epochs):\n",
    "        x_data = x_train[(x_init*batch_size):(x_init+1)*batch_size]\n",
    "        y_data = y_train[(x_init*batch_size):(x_init+1)*batch_size]\n",
    "        x_init = x_init+1\n",
    "        _,epoch_loss = sess.run([optimize,loss],feed_dict={xPh:x_data,yPh:y_data})\n",
    "        print(\"loss\",x_data,\"++\",epoch_loss,x_init)\n",
    "    mFinal,cFinal = sess.run([m,c])\n",
    "    print(mFinal,cFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
